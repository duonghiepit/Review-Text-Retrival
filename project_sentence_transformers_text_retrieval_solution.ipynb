{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt8u6MjFdbU57V7I/8TnG0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duonghiepit/Review-Text-Retrival/blob/main/project_sentence_transformers_text_retrieval_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq6RVM3lV5gs"
      },
      "outputs": [],
      "source": [
        "!pip install datasets sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data_set = load_dataset('ms_marco', 'v1.1')"
      ],
      "metadata": {
        "id": "WdAKk-eGWLpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset = data_set['test']"
      ],
      "metadata": {
        "id": "ur7j2ilsWZnV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries_infos = []\n",
        "queries = []\n",
        "corpus = []\n",
        "\n",
        "for sample in subset:\n",
        "    query_type = sample['query_type']\n",
        "    if query_type != 'entity':\n",
        "        continue\n",
        "    query_id = sample['query_id']\n",
        "    query_str = sample['query']\n",
        "    passages_dict = sample['passages']\n",
        "    is_selected_lst = passages_dict['is_selected']\n",
        "    passage_text_lst = passages_dict['passage_text']\n",
        "    query_info = {\n",
        "        'query_id': query_id,\n",
        "        'query': query_str,\n",
        "        'relevant_docs': []\n",
        "    }\n",
        "    current_len_corpus = len(corpus)\n",
        "    for idx in range(len(is_selected_lst)):\n",
        "        if is_selected_lst[idx] == 1:\n",
        "            doc_idx = current_len_corpus + idx\n",
        "            query_info['relevant_docs'].append(doc_idx)\n",
        "\n",
        "    if query_info['relevant_docs'] == []:\n",
        "        continue\n",
        "\n",
        "    queries.append(query_str)\n",
        "    queries_infos.append(query_info)\n",
        "    corpus += passage_text_lst"
      ],
      "metadata": {
        "id": "X-w4E9LrWqsw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "english_stopwords = stopwords.words('english')\n",
        "remove_chars = string.punctuation\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-Y2ZYnjYiOJ",
        "outputId": "d1c85f70-a8c8-4aea-d0d0-e18dce6d4134"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_normalize(text):\n",
        "    text = text.lower()\n",
        "    for char in remove_chars:\n",
        "        text.replace(char, '')\n",
        "    text = ' '.join([word for word in text.split() if word not in english_stopwords])\n",
        "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "wm-fiCrBZiMi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom search function"
      ],
      "metadata": {
        "id": "VzrxBIPface-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "aA_518a7azXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_queries = ['what is facebook']\n",
        "\n",
        "top_k = min(5, len(corpus))\n",
        "for query in custom_queries:\n",
        "    query_embeddings = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    cos_scores = util.cos_sim(query_embeddings, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    print(\"\\n\\n======================\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"Top 5 most similar sentences in corpus:\\n\")\n",
        "\n",
        "    for idx, (score, doc_idx) in enumerate(zip(top_results[0], top_results[1])):\n",
        "        print(f'Document rank {idx + 1}:')\n",
        "        print(corpus[idx], f'\\n(Score: {score:.4f})', '\\n')"
      ],
      "metadata": {
        "id": "bE6j71iZaiYI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}